<!DOCTYPE html>




<html lang="en">
  <head>
    <meta charset="utf-8" />
    
    <title>4.3. Memory Management &mdash; Numba 0.28.0.dev0+178.g0019cb6-py2.7-linux-x86_64.egg documentation</title>

    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    
<link rel="stylesheet" href="../_static/css/basicstrap-base.css" type="text/css" />
<link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
<link rel="stylesheet" href="../_static/css/font-awesome.min.css">
<!--[if IE 7]>
<link rel="stylesheet" href="../_static/css/font-awesome-ie7.min.css">
<![endif]-->
<style type="text/css">
  body {
    padding-top: 60px;
    padding-bottom: 40px;
  }
}
</style>
<link rel="stylesheet" href="../_static/basicstrap.css" type="text/css" />
<link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
<link rel="stylesheet" href="../_static/css/bootstrap-responsive.min.css" type="text/css" />
    
<script type="text/javascript">
  var DOCUMENTATION_OPTIONS = {
            URL_ROOT:    '../',
            VERSION:     '0.28.0.dev0+178.g0019cb6-py2.7-linux-x86_64.egg',
            COLLAPSE_INDEX: false,
            FILE_SUFFIX: '.html',
            HAS_SOURCE:  true
  };
</script>
<script type="text/javascript" src="../_static/js/jquery.min.js"></script>
<script type="text/javascript" src="../_static/underscore.js"></script>
<script type="text/javascript" src="../_static/doctools.js"></script>
<script type="text/javascript" src="../_static/js/bootstrap.min.js"></script>
<script type="text/javascript">
  $(document).ready(function(){
    $('.show-sidebar').click(function(e) {
       e.preventDefault();
       if ($(".show-sidebar").html() == "Open Table Of Contents") {
          $('.for-mobile').removeClass('hidden-phone');
          $(".show-sidebar").html("Close Table Of Contents");
       } else {
          $(".show-sidebar").html("Open Table Of Contents");
       }
    });
  });
</script>
    <link rel="top" title="Numba 0.28.0.dev0+178.g0019cb6-py2.7-linux-x86_64.egg documentation" href="../index.html" />
    <link rel="up" title="4. CUDA Python Reference" href="index.html" />
    <link rel="next" title="5. Numba for HSA APUs" href="../hsa/index.html" />
    <link rel="prev" title="4.2. CUDA Kernel API" href="kernel.html" /> 
  </head>
  <body>
    <div class="navbar navbar-fixed-top ">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Numba 0.28.0.dev0+178.g0019cb6-py2.7-linux-x86_64.egg documentation</a>
          <div class="nav-collapse collapse">
            <ul class="nav pull-right">
              
                <li>
                <a href="../genindex.html" title="General Index" accesskey="I">index</a>
                </li>
                <li>
                <a href="../py-modindex.html" title="Python Module Index" >modules</a>
                </li>
                <li>
                <a href="../hsa/index.html" title="5. Numba for HSA APUs" accesskey="N">next</a>
                </li>
                <li>
                <a href="kernel.html" title="4.2. CUDA Kernel API" accesskey="P">previous</a>
                </li>
                <li>
                <a href="index.html" accesskey="U">4. CUDA Python Reference</a>
                </li>
              
            </ul>
          </div>
        </div>
      </div>
    </div>
    

    <!-- container -->
    <div class="container-fluid">


      
      <div class="row-fluid hidden-desktop hidden-tablet">
      
<div class="span3 ">
  <a class="visible-phone btn btn-small show-sidebar" data-toggle="collapse" data-target=".for-mobile">Open Table Of Contents</a>
  <div class="for-mobile sidebar hidden-phone">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">4.3. Memory Management</a><ul>
<li><a class="reference internal" href="#device-objects">4.3.1. Device Objects</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="kernel.html"
                        title="previous chapter">4.2. CUDA Kernel API</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../hsa/index.html"
                        title="next chapter">5. Numba for HSA APUs</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/cuda-reference/memory.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox">
  <h3>Quick search</h3>
  <form class="search form-search" action="../search.html" method="get">
      <div class="input-append">
        <input type="text" class="search-query" name="q">
        <input type="submit" class="btn" value="Go" />
      </div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
  </div>
</div>
      </div>
      

      <!-- row -->
      <div class="row-fluid">
         
<div class="span3 visible-desktop visible-tablet">
  <div class=" sidebar hidden-phone">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">4.3. Memory Management</a><ul>
<li><a class="reference internal" href="#device-objects">4.3.1. Device Objects</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="kernel.html"
                        title="previous chapter">4.2. CUDA Kernel API</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../hsa/index.html"
                        title="next chapter">5. Numba for HSA APUs</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/cuda-reference/memory.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox">
  <h3>Quick search</h3>
  <form class="search form-search" action="../search.html" method="get">
      <div class="input-append">
        <input type="text" class="search-query" name="q">
        <input type="submit" class="btn" value="Go" />
      </div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
  </div>
</div> 
        

        <div class="span9">
          <div class="document">
            <div class="documentwrapper">
              <div class="bodywrapper">
                <div class="body">
                  
  <div class="section" id="memory-management">
<h1>4.3. Memory Management<a class="headerlink" href="#memory-management" title="Permalink to this headline">¶</a></h1>
<dl class="function">
<dt id="numba.cuda.to_device">
<code class="descclassname">numba.cuda.</code><code class="descname">to_device</code><span class="sig-paren">(</span><em>obj</em>, <em>stream=0</em>, <em>copy=True</em>, <em>to=None</em><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.to_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Allocate and transfer a numpy ndarray or structured scalar to the device.</p>
<p>To copy host-&gt;device a numpy array:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">ary</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">d_ary</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">ary</span><span class="p">)</span>
</pre></div>
</div>
<p>To enqueue the transfer to a stream:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">stream</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">()</span>
<span class="n">d_ary</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">ary</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="n">stream</span><span class="p">)</span>
</pre></div>
</div>
<p>The resulting <code class="docutils literal"><span class="pre">d_ary</span></code> is a <code class="docutils literal"><span class="pre">DeviceNDArray</span></code>.</p>
<p>To copy device-&gt;host:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">hary</span> <span class="o">=</span> <span class="n">d_ary</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">()</span>
</pre></div>
</div>
<p>To copy device-&gt;host to an existing array:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">ary</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">d_ary</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">d_ary</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">d_ary</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">(</span><span class="n">ary</span><span class="p">)</span>
</pre></div>
</div>
<p>To enqueue the transfer to a stream:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">hary</span> <span class="o">=</span> <span class="n">d_ary</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">(</span><span class="n">stream</span><span class="o">=</span><span class="n">stream</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="numba.cuda.device_array">
<code class="descclassname">numba.cuda.</code><code class="descname">device_array</code><span class="sig-paren">(</span><em>shape</em>, <em>dtype=np.float</em>, <em>strides=None</em>, <em>order='C'</em>, <em>stream=0</em><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.device_array" title="Permalink to this definition">¶</a></dt>
<dd><p>Allocate an empty device ndarray. Similar to <code class="xref py py-meth docutils literal"><span class="pre">numpy.empty()</span></code>.</p>
</dd></dl>

<dl class="function">
<dt id="numba.cuda.device_array_like">
<code class="descclassname">numba.cuda.</code><code class="descname">device_array_like</code><span class="sig-paren">(</span><em>ary</em>, <em>stream=0</em><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.device_array_like" title="Permalink to this definition">¶</a></dt>
<dd><p>Call cuda.devicearray() with information from the array.</p>
</dd></dl>

<dl class="function">
<dt id="numba.cuda.pinned_array">
<code class="descclassname">numba.cuda.</code><code class="descname">pinned_array</code><span class="sig-paren">(</span><em>shape</em>, <em>dtype=np.float</em>, <em>strides=None</em>, <em>order='C'</em><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.pinned_array" title="Permalink to this definition">¶</a></dt>
<dd><p>Allocate a np.ndarray with a buffer that is pinned (pagelocked).
Similar to np.empty().</p>
</dd></dl>

<dl class="function">
<dt id="numba.cuda.mapped_array">
<code class="descclassname">numba.cuda.</code><code class="descname">mapped_array</code><span class="sig-paren">(</span><em>shape</em>, <em>dtype=np.float</em>, <em>strides=None</em>, <em>order='C'</em>, <em>stream=0</em>, <em>portable=False</em>, <em>wc=False</em><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.mapped_array" title="Permalink to this definition">¶</a></dt>
<dd><p>Allocate a mapped ndarray with a buffer that is pinned and mapped on
to the device. Similar to np.empty()</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>portable</strong> &#8211; a boolean flag to allow the allocated device memory to be
usable in multiple devices.</li>
<li><strong>wc</strong> &#8211; a boolean flag to enable writecombined allocation which is faster
to write by the host and to read by the device, but slower to
write by the host and slower to write by the device.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="numba.cuda.pinned">
<code class="descclassname">numba.cuda.</code><code class="descname">pinned</code><span class="sig-paren">(</span><em>*args</em>, <em>**kws</em><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.pinned" title="Permalink to this definition">¶</a></dt>
<dd><p>A context manager for temporary pinning a sequence of host ndarrays.</p>
</dd></dl>

<dl class="function">
<dt id="numba.cuda.mapped">
<code class="descclassname">numba.cuda.</code><code class="descname">mapped</code><span class="sig-paren">(</span><em>*args</em>, <em>**kws</em><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.mapped" title="Permalink to this definition">¶</a></dt>
<dd><p>A context manager for temporarily mapping a sequence of host ndarrays.</p>
</dd></dl>

<div class="section" id="device-objects">
<h2>4.3.1. Device Objects<a class="headerlink" href="#device-objects" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numba.cuda.cudadrv.devicearray.DeviceNDArray">
<em class="property">class </em><code class="descclassname">numba.cuda.cudadrv.devicearray.</code><code class="descname">DeviceNDArray</code><span class="sig-paren">(</span><em>shape</em>, <em>strides</em>, <em>dtype</em>, <em>stream=0</em>, <em>writeback=None</em>, <em>gpu_data=None</em><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.cudadrv.devicearray.DeviceNDArray" title="Permalink to this definition">¶</a></dt>
<dd><p>An on-GPU array type</p>
<dl class="method">
<dt id="numba.cuda.cudadrv.devicearray.DeviceNDArray.copy_to_device">
<code class="descname">copy_to_device</code><span class="sig-paren">(</span><em>ary</em>, <em>stream=0</em><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.cudadrv.devicearray.DeviceNDArray.copy_to_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy <cite>ary</cite> to <cite>self</cite>.</p>
<p>If <cite>ary</cite> is a CUDA memory, perform a device-to-device transfer.
Otherwise, perform a a host-to-device transfer.</p>
</dd></dl>

<dl class="method">
<dt id="numba.cuda.cudadrv.devicearray.DeviceNDArray.copy_to_host">
<code class="descname">copy_to_host</code><span class="sig-paren">(</span><em>ary=None</em>, <em>stream=0</em><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.cudadrv.devicearray.DeviceNDArray.copy_to_host" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy <code class="docutils literal"><span class="pre">self</span></code> to <code class="docutils literal"><span class="pre">ary</span></code> or create a new Numpy ndarray
if <code class="docutils literal"><span class="pre">ary</span></code> is <code class="docutils literal"><span class="pre">None</span></code>.</p>
<p>If a CUDA <code class="docutils literal"><span class="pre">stream</span></code> is given, then the transfer will be made
asynchronously as part as the given stream.  Otherwise, the transfer is
synchronous: the function returns after the copy is finished.</p>
<p>Always returns the host array.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">cuda</span>

<span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">d_arr</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>

<span class="n">my_kernel</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">](</span><span class="n">d_arr</span><span class="p">)</span>

<span class="n">result_array</span> <span class="o">=</span> <span class="n">d_arr</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="numba.cuda.cudadrv.devicearray.DeviceNDArray.is_c_contiguous">
<code class="descname">is_c_contiguous</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.cudadrv.devicearray.DeviceNDArray.is_c_contiguous" title="Permalink to this definition">¶</a></dt>
<dd><p>Return true if the array is C-contiguous.</p>
</dd></dl>

<dl class="method">
<dt id="numba.cuda.cudadrv.devicearray.DeviceNDArray.is_f_contiguous">
<code class="descname">is_f_contiguous</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.cudadrv.devicearray.DeviceNDArray.is_f_contiguous" title="Permalink to this definition">¶</a></dt>
<dd><p>Return true if the array is Fortran-contiguous.</p>
</dd></dl>

<dl class="method">
<dt id="numba.cuda.cudadrv.devicearray.DeviceNDArray.ravel">
<code class="descname">ravel</code><span class="sig-paren">(</span><em>order='C'</em>, <em>stream=0</em><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.cudadrv.devicearray.DeviceNDArray.ravel" title="Permalink to this definition">¶</a></dt>
<dd><p>Flatten the array without changing its contents, similar to
<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.ravel.html#numpy.ndarray.ravel" title="(in NumPy v1.11)"><code class="xref py py-meth docutils literal"><span class="pre">numpy.ndarray.ravel()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="numba.cuda.cudadrv.devicearray.DeviceNDArray.reshape">
<code class="descname">reshape</code><span class="sig-paren">(</span><em>*newshape</em>, <em>**kws</em><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.cudadrv.devicearray.DeviceNDArray.reshape" title="Permalink to this definition">¶</a></dt>
<dd><p>Reshape the array without changing its contents, similarly to
<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.reshape.html#numpy.ndarray.reshape" title="(in NumPy v1.11)"><code class="xref py py-meth docutils literal"><span class="pre">numpy.ndarray.reshape()</span></code></a>. Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">d_arr</span> <span class="o">=</span> <span class="n">d_arr</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s">&#39;F&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="numba.cuda.cudadrv.devicearray.DeviceNDArray.split">
<code class="descname">split</code><span class="sig-paren">(</span><em>section</em>, <em>stream=0</em><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.cudadrv.devicearray.DeviceNDArray.split" title="Permalink to this definition">¶</a></dt>
<dd><p>Split the array into equal partition of the <cite>section</cite> size.
If the array cannot be equally divided, the last section will be
smaller.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="numba.cuda.cudadrv.devicearray.DeviceRecord">
<em class="property">class </em><code class="descclassname">numba.cuda.cudadrv.devicearray.</code><code class="descname">DeviceRecord</code><span class="sig-paren">(</span><em>dtype</em>, <em>stream=0</em>, <em>gpu_data=None</em><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.cudadrv.devicearray.DeviceRecord" title="Permalink to this definition">¶</a></dt>
<dd><p>An on-GPU record type</p>
<dl class="method">
<dt id="numba.cuda.cudadrv.devicearray.DeviceRecord.copy_to_device">
<code class="descname">copy_to_device</code><span class="sig-paren">(</span><em>ary</em>, <em>stream=0</em><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.cudadrv.devicearray.DeviceRecord.copy_to_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy <cite>ary</cite> to <cite>self</cite>.</p>
<p>If <cite>ary</cite> is a CUDA memory, perform a device-to-device transfer.
Otherwise, perform a a host-to-device transfer.</p>
</dd></dl>

<dl class="method">
<dt id="numba.cuda.cudadrv.devicearray.DeviceRecord.copy_to_host">
<code class="descname">copy_to_host</code><span class="sig-paren">(</span><em>ary=None</em>, <em>stream=0</em><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.cudadrv.devicearray.DeviceRecord.copy_to_host" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy <code class="docutils literal"><span class="pre">self</span></code> to <code class="docutils literal"><span class="pre">ary</span></code> or create a new Numpy ndarray
if <code class="docutils literal"><span class="pre">ary</span></code> is <code class="docutils literal"><span class="pre">None</span></code>.</p>
<p>If a CUDA <code class="docutils literal"><span class="pre">stream</span></code> is given, then the transfer will be made
asynchronously as part as the given stream.  Otherwise, the transfer is
synchronous: the function returns after the copy is finished.</p>
<p>Always returns the host array.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">cuda</span>

<span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">d_arr</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>

<span class="n">my_kernel</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">](</span><span class="n">d_arr</span><span class="p">)</span>

<span class="n">result_array</span> <span class="o">=</span> <span class="n">d_arr</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="numba.cuda.cudadrv.devicearray.MappedNDArray">
<em class="property">class </em><code class="descclassname">numba.cuda.cudadrv.devicearray.</code><code class="descname">MappedNDArray</code><span class="sig-paren">(</span><em>shape</em>, <em>strides</em>, <em>dtype</em>, <em>stream=0</em>, <em>writeback=None</em>, <em>gpu_data=None</em><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.cudadrv.devicearray.MappedNDArray" title="Permalink to this definition">¶</a></dt>
<dd><p>A host array that uses CUDA mapped memory.</p>
<dl class="method">
<dt id="numba.cuda.cudadrv.devicearray.MappedNDArray.copy_to_device">
<code class="descname">copy_to_device</code><span class="sig-paren">(</span><em>ary</em>, <em>stream=0</em><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.cudadrv.devicearray.MappedNDArray.copy_to_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy <cite>ary</cite> to <cite>self</cite>.</p>
<p>If <cite>ary</cite> is a CUDA memory, perform a device-to-device transfer.
Otherwise, perform a a host-to-device transfer.</p>
</dd></dl>

<dl class="method">
<dt id="numba.cuda.cudadrv.devicearray.MappedNDArray.copy_to_host">
<code class="descname">copy_to_host</code><span class="sig-paren">(</span><em>ary=None</em>, <em>stream=0</em><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.cudadrv.devicearray.MappedNDArray.copy_to_host" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy <code class="docutils literal"><span class="pre">self</span></code> to <code class="docutils literal"><span class="pre">ary</span></code> or create a new Numpy ndarray
if <code class="docutils literal"><span class="pre">ary</span></code> is <code class="docutils literal"><span class="pre">None</span></code>.</p>
<p>If a CUDA <code class="docutils literal"><span class="pre">stream</span></code> is given, then the transfer will be made
asynchronously as part as the given stream.  Otherwise, the transfer is
synchronous: the function returns after the copy is finished.</p>
<p>Always returns the host array.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">cuda</span>

<span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">d_arr</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>

<span class="n">my_kernel</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">](</span><span class="n">d_arr</span><span class="p">)</span>

<span class="n">result_array</span> <span class="o">=</span> <span class="n">d_arr</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="numba.cuda.cudadrv.devicearray.MappedNDArray.split">
<code class="descname">split</code><span class="sig-paren">(</span><em>section</em>, <em>stream=0</em><span class="sig-paren">)</span><a class="headerlink" href="#numba.cuda.cudadrv.devicearray.MappedNDArray.split" title="Permalink to this definition">¶</a></dt>
<dd><p>Split the array into equal partition of the <cite>section</cite> size.
If the array cannot be equally divided, the last section will be
smaller.</p>
</dd></dl>

</dd></dl>

</div>
</div>


                </div>
              </div>
            </div>
          </div>
        </div>
        
        
      </div><!-- /row -->

      <!-- row -->
      <div class="row-fluid">
<div class="related navbar ">
  <div class="navbar-inner">
    <ul class="nav pull-right">
      
        <li><a href="../genindex.html" title="General Index" >index</a></li>
        <li><a href="../py-modindex.html" title="Python Module Index" >modules</a></li>
        <li><a href="../hsa/index.html" title="5. Numba for HSA APUs" >next</a></li>
        <li><a href="kernel.html" title="4.2. CUDA Kernel API" >previous</a></li>
        <li><a href="../index.html">Numba 0.28.0.dev0+178.g0019cb6-py2.7-linux-x86_64.egg documentation</a></li>
        <li><a href="index.html" >4. CUDA Python Reference</a></li> 
      
    </ul>
  </div>
</div>
      </div><!-- /row -->

      <!-- footer -->
      <footer>
          &copy; Copyright 2012-2015, Continuum Analytics.
        Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.3.1.
      </footer>
      <!-- /footer -->

    </div>
    <!-- /container -->

  </body>
</html>