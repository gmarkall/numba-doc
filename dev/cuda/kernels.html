<!DOCTYPE html>




<html lang="en">
  <head>
    <meta charset="utf-8" />
    
    <title>3.2. Writing CUDA Kernels &mdash; Numba 0.18.2_273_g06952c7-py2.7-linux-x86_64.egg documentation</title>

    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    
<link rel="stylesheet" href="../_static/css/basicstrap-base.css" type="text/css" />
<link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
<link rel="stylesheet" href="../_static/css/font-awesome.min.css">
<!--[if IE 7]>
<link rel="stylesheet" href="../_static/css/font-awesome-ie7.min.css">
<![endif]-->
<style type="text/css">
  body {
    padding-top: 60px;
    padding-bottom: 40px;
  }
}
</style>
<link rel="stylesheet" href="../_static/basicstrap.css" type="text/css" />
<link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
<link rel="stylesheet" href="../_static/css/bootstrap-responsive.min.css" type="text/css" />
    
<script type="text/javascript">
  var DOCUMENTATION_OPTIONS = {
            URL_ROOT:    '../',
            VERSION:     '0.18.2_273_g06952c7-py2.7-linux-x86_64.egg',
            COLLAPSE_INDEX: false,
            FILE_SUFFIX: '.html',
            HAS_SOURCE:  true
  };
</script>
<script type="text/javascript" src="../_static/js/jquery.min.js"></script>
<script type="text/javascript" src="../_static/underscore.js"></script>
<script type="text/javascript" src="../_static/doctools.js"></script>
<script type="text/javascript" src="../_static/js/bootstrap.min.js"></script>
<script type="text/javascript">
  $(document).ready(function(){
    $('.show-sidebar').click(function(e) {
       e.preventDefault();
       if ($(".show-sidebar").html() == "Open Table Of Contents") {
          $('.for-mobile').removeClass('hidden-phone');
          $(".show-sidebar").html("Close Table Of Contents");
       } else {
          $(".show-sidebar").html("Open Table Of Contents");
       }
    });
  });
</script>
    <link rel="top" title="Numba 0.18.2_273_g06952c7-py2.7-linux-x86_64.egg documentation" href="../index.html" />
    <link rel="up" title="3. Numba for CUDA GPUs" href="index.html" />
    <link rel="next" title="3.3. Memory management" href="memory.html" />
    <link rel="prev" title="3.1. Overview" href="overview.html" /> 
  </head>
  <body>
    <div class="navbar navbar-fixed-top ">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Numba 0.18.2_273_g06952c7-py2.7-linux-x86_64.egg documentation</a>
          <div class="nav-collapse collapse">
            <ul class="nav pull-right">
              
                <li>
                <a href="../genindex.html" title="General Index" accesskey="I">index</a>
                </li>
                <li>
                <a href="../py-modindex.html" title="Python Module Index" >modules</a>
                </li>
                <li>
                <a href="memory.html" title="3.3. Memory management" accesskey="N">next</a>
                </li>
                <li>
                <a href="overview.html" title="3.1. Overview" accesskey="P">previous</a>
                </li>
                <li>
                <a href="index.html" accesskey="U">3. Numba for CUDA GPUs</a>
                </li>
              
            </ul>
          </div>
        </div>
      </div>
    </div>
    

    <!-- container -->
    <div class="container-fluid">


      
      <div class="row-fluid hidden-desktop hidden-tablet">
      
<div class="span3 ">
  <a class="visible-phone btn btn-small show-sidebar" data-toggle="collapse" data-target=".for-mobile">Open Table Of Contents</a>
  <div class="for-mobile sidebar hidden-phone">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">3.2. Writing CUDA Kernels</a><ul>
<li><a class="reference internal" href="#introduction">3.2.1. Introduction</a></li>
<li><a class="reference internal" href="#kernel-declaration">3.2.2. Kernel declaration</a></li>
<li><a class="reference internal" href="#kernel-invocation">3.2.3. Kernel invocation</a><ul>
<li><a class="reference internal" href="#choosing-the-block-size">3.2.3.1. Choosing the block size</a></li>
<li><a class="reference internal" href="#multi-dimensional-blocks-and-grids">3.2.3.2. Multi-dimensional blocks and grids</a></li>
</ul>
</li>
<li><a class="reference internal" href="#thread-positioning">3.2.4. Thread positioning</a><ul>
<li><a class="reference internal" href="#absolute-positions">3.2.4.1. Absolute positions</a></li>
<li><a class="reference internal" href="#further-reading">3.2.4.2. Further Reading</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="overview.html"
                        title="previous chapter">3.1. Overview</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="memory.html"
                        title="next chapter">3.3. Memory management</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="../_sources/cuda/kernels.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox">
  <h3>Quick search</h3>
  <form class="search form-search" action="../search.html" method="get">
      <div class="input-append">
        <input type="text" class="search-query" name="q">
        <input type="submit" class="btn" value="Go" />
      </div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
  </div>
</div>
      </div>
      

      <!-- row -->
      <div class="row-fluid">
         
<div class="span3 visible-desktop visible-tablet">
  <div class=" sidebar hidden-phone">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">3.2. Writing CUDA Kernels</a><ul>
<li><a class="reference internal" href="#introduction">3.2.1. Introduction</a></li>
<li><a class="reference internal" href="#kernel-declaration">3.2.2. Kernel declaration</a></li>
<li><a class="reference internal" href="#kernel-invocation">3.2.3. Kernel invocation</a><ul>
<li><a class="reference internal" href="#choosing-the-block-size">3.2.3.1. Choosing the block size</a></li>
<li><a class="reference internal" href="#multi-dimensional-blocks-and-grids">3.2.3.2. Multi-dimensional blocks and grids</a></li>
</ul>
</li>
<li><a class="reference internal" href="#thread-positioning">3.2.4. Thread positioning</a><ul>
<li><a class="reference internal" href="#absolute-positions">3.2.4.1. Absolute positions</a></li>
<li><a class="reference internal" href="#further-reading">3.2.4.2. Further Reading</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="overview.html"
                        title="previous chapter">3.1. Overview</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="memory.html"
                        title="next chapter">3.3. Memory management</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="../_sources/cuda/kernels.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox">
  <h3>Quick search</h3>
  <form class="search form-search" action="../search.html" method="get">
      <div class="input-append">
        <input type="text" class="search-query" name="q">
        <input type="submit" class="btn" value="Go" />
      </div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
  </div>
</div> 
        

        <div class="span9">
          <div class="document">
            <div class="documentwrapper">
              <div class="bodywrapper">
                <div class="body">
                  
  <div class="section" id="writing-cuda-kernels">
<h1>3.2. Writing CUDA Kernels<a class="headerlink" href="#writing-cuda-kernels" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>3.2.1. Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>CUDA has an execution model unlike the traditional sequential model used
for programming CPUs.  In CUDA, the code you write will be executed by
multiple threads at once (often hundreds or thousands).  Your solution will
be modeled by defining a thread hierarchy of <em>grid</em>, <em>blocks</em> and <em>threads</em>.</p>
<p>Numba&#8217;s CUDA support exposes facilities to declare and manage this
hierarchy of threads.  The facilities are largely similar to those
exposed by NVidia&#8217;s CUDA C language.</p>
<p>Numba also exposes three kinds of GPU memory: global <a class="reference internal" href="memory.html#cuda-device-memory"><em>device memory</em></a> (the large, relatively slow
off-chip memory that&#8217;s connected to the GPU itself), on-chip
<a class="reference internal" href="memory.html#cuda-shared-memory"><em>shared memory</em></a> and <a class="reference internal" href="memory.html#cuda-local-memory"><em>local memory</em></a>.
For all but the simplest algorithms, it is important that you carefully
consider how to use and access memory in order to minimize bandwidth
requirements and contention.</p>
</div>
<div class="section" id="kernel-declaration">
<h2>3.2.2. Kernel declaration<a class="headerlink" href="#kernel-declaration" title="Permalink to this headline">¶</a></h2>
<p>A <em>kernel function</em> is a GPU function that is meant to be called from CPU
code (*).  It gives it two fundamental characteristics:</p>
<ul class="simple">
<li>kernels cannot explicitly return a value; all result data must be written
to an array passed to the function (if computing a scalar, you will
probably pass a one-element array);</li>
<li>kernels explicitly declare their thread hierarchy when called: i.e.
the number of thread blocks and the number of threads per block
(note that while a kernel is compiled once, it can be called multiple
times with different block sizes or grid sizes).</li>
</ul>
<p>At first sight, writing a CUDA kernel with Numba looks very much like
writing a <a class="reference internal" href="../glossary.html#term-jit-function"><em class="xref std std-term">JIT function</em></a> for the CPU:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="nd">@cuda.jit</span>
<span class="k">def</span> <span class="nf">increment_by_one</span><span class="p">(</span><span class="n">an_array</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Increment all array elements by one.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c"># code elided here; read further for different implementations</span>
</pre></div>
</div>
<p>(*) Note: newer CUDA devices support device-side kernel launching; this feature
is called <em>dynamic parallelism</em> but Numba does not support it currently)</p>
</div>
<div class="section" id="kernel-invocation">
<span id="cuda-kernel-invocation"></span><h2>3.2.3. Kernel invocation<a class="headerlink" href="#kernel-invocation" title="Permalink to this headline">¶</a></h2>
<p>A kernel is typically launched in the following way:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">threadsperblock</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">blockspergrid</span> <span class="o">=</span> <span class="p">(</span><span class="n">an_array</span><span class="o">.</span><span class="n">size</span> <span class="o">+</span> <span class="p">(</span><span class="n">threadsperblock</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">//</span> <span class="n">threadperblock</span>
<span class="n">increment_by_one</span><span class="p">[</span><span class="n">blockspergrid</span><span class="p">,</span> <span class="n">threadsperblock</span><span class="p">](</span><span class="n">an_array</span><span class="p">)</span>
</pre></div>
</div>
<p>We notice two steps here:</p>
<ul class="simple">
<li>Instantiate the kernel proper, by specifying a number of blocks
(or &#8220;blocks per grid&#8221;), and a number of threads per block.  The product
of the two will give the total number of threads launched.  Kernel
instantiation is done by taking the compiled kernel function
(here <tt class="docutils literal"><span class="pre">increment_by_one</span></tt>) and indexing it with a tuple of integers.</li>
<li>Running the kernel, by passing it the input array (and any separate
output arrays if necessary).  By default, running a kernel is synchronous:
the function returns when the kernel has finished executing and the
data is synchronized back.</li>
</ul>
<div class="section" id="choosing-the-block-size">
<h3>3.2.3.1. Choosing the block size<a class="headerlink" href="#choosing-the-block-size" title="Permalink to this headline">¶</a></h3>
<p>It might seem curious to have a two-level hierarchy when declaring the
number of threads needed by a kernel.  The block size (i.e. number of
threads per block) is often crucial:</p>
<ul class="simple">
<li>On the software side, the block size determines how many threads
share a given area of <a class="reference internal" href="memory.html#cuda-shared-memory"><em>shared memory</em></a>.</li>
<li>On the hardware side, the block size must be large enough for full
occupation of execution units; recommendations can be found in the
<a class="reference external" href="http://docs.nvidia.com/cuda/cuda-c-programming-guide">CUDA C Programming Guide</a>.</li>
</ul>
</div>
<div class="section" id="multi-dimensional-blocks-and-grids">
<h3>3.2.3.2. Multi-dimensional blocks and grids<a class="headerlink" href="#multi-dimensional-blocks-and-grids" title="Permalink to this headline">¶</a></h3>
<p>To help deal with multi-dimensional arrays, CUDA allows you to specify
multi-dimensional blocks and grids.  In the example above, you could
make <tt class="docutils literal"><span class="pre">blockspergrid</span></tt> and <tt class="docutils literal"><span class="pre">threadsperblock</span></tt> tuples of one, two
or three integers.  Compared to 1D declarations of equivalent sizes,
this doesn&#8217;t change anything to the efficiency or behaviour of generated
code, but can help you write your algorithms in a more natural way.</p>
</div>
</div>
<div class="section" id="thread-positioning">
<h2>3.2.4. Thread positioning<a class="headerlink" href="#thread-positioning" title="Permalink to this headline">¶</a></h2>
<p>When running a kernel, the kernel function&#8217;s code is executed by every
thread once.  It therefore has to know which thread it is in, in order
to know which array element(s) it is responsible for (complex algorithms
may define more complex responsibilities, but the underlying principle
is the same).</p>
<p>One way is for the thread to determines its position in the grid and block
and manually compute the corresponding array position:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="nd">@cuda.jit</span>
<span class="k">def</span> <span class="nf">increment_by_one</span><span class="p">(</span><span class="n">an_array</span><span class="p">):</span>
    <span class="c"># Thread id in a 1D block</span>
    <span class="n">tx</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span>
    <span class="c"># Block id in a 1D grid</span>
    <span class="n">ty</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">x</span>
    <span class="c"># Block width, i.e. number of threads per block</span>
    <span class="n">bw</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">x</span>
    <span class="c"># Compute flattened index inside the array</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">tx</span> <span class="o">+</span> <span class="n">ty</span> <span class="o">*</span> <span class="n">bw</span>
    <span class="k">if</span> <span class="n">pos</span> <span class="o">&lt;</span> <span class="n">an_array</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>  <span class="c"># Check array boundaries</span>
        <span class="n">an_array</span><span class="p">[</span><span class="n">pos</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Unless you are sure the block size and grid size is a divisor
of your array size, you <strong>must</strong> check boundaries as shown above.</p>
</div>
<p><a class="reference internal" href="#numba.cuda.threadIdx" title="numba.cuda.threadIdx"><tt class="xref py py-attr docutils literal"><span class="pre">threadIdx</span></tt></a>, <a class="reference internal" href="#numba.cuda.blockIdx" title="numba.cuda.blockIdx"><tt class="xref py py-attr docutils literal"><span class="pre">blockIdx</span></tt></a>, <a class="reference internal" href="#numba.cuda.blockDim" title="numba.cuda.blockDim"><tt class="xref py py-attr docutils literal"><span class="pre">blockDim</span></tt></a> and <a class="reference internal" href="#numba.cuda.gridDim" title="numba.cuda.gridDim"><tt class="xref py py-attr docutils literal"><span class="pre">gridDim</span></tt></a>
are special objects provided by the CUDA backend for the sole purpose of
knowing the geometry of the thread hierarchy and the position of the
current thread within that geometry.</p>
<p>These objects can be 1D, 2D or 3D, depending on how the kernel was
<a class="reference internal" href="#cuda-kernel-invocation"><em>invoked</em></a>.  To access the value at each
dimension, use the <tt class="docutils literal"><span class="pre">x</span></tt>, <tt class="docutils literal"><span class="pre">y</span></tt> and <tt class="docutils literal"><span class="pre">z</span></tt> attributes of these objects,
respectively.</p>
<dl class="attribute">
<dt id="numba.cuda.threadIdx">
<tt class="descclassname">numba.cuda.</tt><tt class="descname">threadIdx</tt><a class="headerlink" href="#numba.cuda.threadIdx" title="Permalink to this definition">¶</a></dt>
<dd><p>The thread indices in the current thread block.  For 1D blocks, the index
(given by the <tt class="docutils literal"><span class="pre">x</span></tt> attribute) is an integer spanning the range from 0
inclusive to <a class="reference internal" href="#numba.cuda.blockDim" title="numba.cuda.blockDim"><tt class="xref py py-attr docutils literal"><span class="pre">numba.cuda.blockDim</span></tt></a> exclusive.  A similar rule
exists for each dimension when more than one dimension is used.</p>
</dd></dl>

<dl class="attribute">
<dt id="numba.cuda.blockDim">
<tt class="descclassname">numba.cuda.</tt><tt class="descname">blockDim</tt><a class="headerlink" href="#numba.cuda.blockDim" title="Permalink to this definition">¶</a></dt>
<dd><p>The shape of the block of threads, as declared when instantiating the
kernel.  This value is the same for all threads in a given kernel, even
if they belong to different blocks (i.e. each block is &#8220;full&#8221;).</p>
</dd></dl>

<dl class="attribute">
<dt id="numba.cuda.blockIdx">
<tt class="descclassname">numba.cuda.</tt><tt class="descname">blockIdx</tt><a class="headerlink" href="#numba.cuda.blockIdx" title="Permalink to this definition">¶</a></dt>
<dd><p>The block indices in the grid of threads launched a kernel.  For a 1D grid,
the index (given by the <tt class="docutils literal"><span class="pre">x</span></tt> attribute) is an integer spanning the range
from 0 inclusive to <a class="reference internal" href="#numba.cuda.gridDim" title="numba.cuda.gridDim"><tt class="xref py py-attr docutils literal"><span class="pre">numba.cuda.gridDim</span></tt></a> exclusive.  A similar rule
exists for each dimension when more than one dimension is used.</p>
</dd></dl>

<dl class="attribute">
<dt id="numba.cuda.gridDim">
<tt class="descclassname">numba.cuda.</tt><tt class="descname">gridDim</tt><a class="headerlink" href="#numba.cuda.gridDim" title="Permalink to this definition">¶</a></dt>
<dd><p>The shape of the grid of blocks, i.e. the total number of blocks launched
by this kernel invocation, as declared when instantiating the kernel.</p>
</dd></dl>

<div class="section" id="absolute-positions">
<h3>3.2.4.1. Absolute positions<a class="headerlink" href="#absolute-positions" title="Permalink to this headline">¶</a></h3>
<p>Simple algorithms will tend to always use thread indices in the
same way as shown in the example above.  Numba provides additional facilities
to automate such calculations:</p>
<dl class="function">
<dt id="numba.cuda.grid">
<tt class="descclassname">numba.cuda.</tt><tt class="descname">grid</tt><big>(</big><em>ndim</em><big>)</big><a class="headerlink" href="#numba.cuda.grid" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the absolute position of the current thread in the entire
grid of blocks.  <em>ndim</em> should correspond to the number of dimensions
declared when instantiating the kernel.  If <em>ndim</em> is 1, a single integer
is returned.  If <em>ndim</em> is 2 or 3, a tuple of the given number of
integers is returned.</p>
</dd></dl>

<dl class="function">
<dt id="numba.cuda.gridsize">
<tt class="descclassname">numba.cuda.</tt><tt class="descname">gridsize</tt><big>(</big><em>ndim</em><big>)</big><a class="headerlink" href="#numba.cuda.gridsize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the absolute size (or shape) in threads of the entire grid of
blocks.  <em>ndim</em> has the same meaning as in <a class="reference internal" href="#numba.cuda.grid" title="numba.cuda.grid"><tt class="xref py py-func docutils literal"><span class="pre">grid()</span></tt></a> above.</p>
</dd></dl>

<p>With these functions, the incrementation example can become:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="nd">@cuda.jit</span>
<span class="k">def</span> <span class="nf">increment_by_one</span><span class="p">(</span><span class="n">an_array</span><span class="p">):</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">pos</span> <span class="o">&lt;</span> <span class="n">an_array</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="n">an_array</span><span class="p">[</span><span class="n">pos</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
<p>The same example for a 2D array and grid of threads would be:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="nd">@cuda.jit</span>
<span class="k">def</span> <span class="nf">increment_a_2D_array</span><span class="p">(</span><span class="n">an_array</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">an_array</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="n">an_array</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
       <span class="n">an_array</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
<p>Note the grid computation when instantiating the kernel must still be
done manually, for example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>  <span class="c"># for Python 2</span>

<span class="n">threadsperblock</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">blockspergrid_x</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">an_array</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">threadsperblock</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">blockspergrid_y</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">an_array</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">threadsperblock</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">blockspergrid</span> <span class="o">=</span> <span class="p">(</span><span class="n">blockspergrid_x</span><span class="p">,</span> <span class="n">blockspergrid_y</span><span class="p">)</span>
<span class="n">increment_a_2D_array</span><span class="p">[</span><span class="n">blockspergrid</span><span class="p">,</span> <span class="n">threadsperblock</span><span class="p">](</span><span class="n">an_array</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="further-reading">
<h3>3.2.4.2. Further Reading<a class="headerlink" href="#further-reading" title="Permalink to this headline">¶</a></h3>
<p>Please refer to the the <a class="reference external" href="http://docs.nvidia.com/cuda/cuda-c-programming-guide">CUDA C Programming Guide</a> for a detailed discussion
of CUDA programming.</p>
</div>
</div>
</div>


                </div>
              </div>
            </div>
          </div>
        </div>
        
        
      </div><!-- /row -->

      <!-- row -->
      <div class="row-fluid">
<div class="related navbar ">
  <div class="navbar-inner">
    <ul class="nav pull-right">
      
        <li><a href="../genindex.html" title="General Index" >index</a></li>
        <li><a href="../py-modindex.html" title="Python Module Index" >modules</a></li>
        <li><a href="memory.html" title="3.3. Memory management" >next</a></li>
        <li><a href="overview.html" title="3.1. Overview" >previous</a></li>
        <li><a href="../index.html">Numba 0.18.2_273_g06952c7-py2.7-linux-x86_64.egg documentation</a></li>
        <li><a href="index.html" >3. Numba for CUDA GPUs</a></li> 
      
    </ul>
  </div>
</div>
      </div><!-- /row -->

      <!-- footer -->
      <footer>
          &copy; Copyright 2012-2014, Continuum Analytics.
        Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.2.2.
      </footer>
      <!-- /footer -->

    </div>
    <!-- /container -->

  </body>
</html>